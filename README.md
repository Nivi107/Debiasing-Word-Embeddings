# Debiasing Pretrained Word Embeddings
In recent years, sentiment analysis models have become essential for understanding public opinion across various domains. However, these models often inherit biases from their training data and pretrained word embeddings, such as Word2Vec or GloVe, leading to skewed predictions that perpetuate social stereotypes. A pressing issue in machine learning research is to assess whether automated algorithms, including those used in sentiment analysis, discriminate based on gender or race when making decisions. This project addresses gender bias in sentiment analysis by employing debiasing techniques on pretrained word embeddings to reduce bias while maintaining model performance.

The project follows a structured approach, beginning with the collection and preprocessing of a dataset known to contain gender bias. A baseline sentiment analysis model is trained, and its predictions are tested for gender bias using the Equity Evaluation Corpus (EEC), specifically analyzing disparities in predicting the emotion of anger for male and female subjects. The results offer a comprehensive analysis of the effectiveness of debiasing techniques in mitigating bias without significantly compromising accuracy. These findings contribute to the broader discussion on ethical AI development, emphasizing the need for fairness in machine learning models.
